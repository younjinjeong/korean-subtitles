1
00:00:00,000 --> 00:00:04,660
넷플릭스 캐싱 - 숨겨진 마이크로 서비스
(자막 번역: 정윤진, yjeong@pivotal.io)

2
00:00:04,660 --> 00:00:06,280
모두 안녕하세요~

3
00:00:06,280 --> 00:00:07,840
제 이름은 스캇 맨스필드구요

4
00:00:07,900 --> 00:00:10,740
전 넷플릭스 EVCache팀의 시니어 소프트웨어 엔지니어 입니다

5
00:00:10,800 --> 00:00:14,240
오늘 저는 여러분들께 '숨겨진 마이크로 서비스'에 대해 이야기 할거에요

6
00:00:14,340 --> 00:00:16,540
'숨겨진 마이크로 서비스'는 저희 캐시 계층을 뜻하는 별명 입니다

7
00:00:16,580 --> 00:00:18,700
시작하기 전에 확실히 하고 싶은것은,

8
00:00:18,700 --> 00:00:26,500
다른 형태의 멋진 캐시 서비스인 이건 '오픈 커넥트' CDN과는 아무 관계 없구요

9
00:00:26,500 --> 00:00:28,120
오늘의 주제가 아닙니다

10
00:00:28,200 --> 00:00:32,980
오늘의 주제는 AWS에서 동작하는 캐시 계층에 대한 것입니다

11
00:00:35,280 --> 00:00:39,120
여러분은 음, 넷플릭스에 접속하시겠죠? 좋은 결정입니다

12
00:00:39,180 --> 00:00:42,200
퇴근해서 집에 도착하시면 티비를 틀고,

13
00:00:42,360 --> 00:00:44,620
넷플릭스 앱을 구동합니다

14
00:00:44,620 --> 00:00:48,220
새로 만드신 계정으로 로그인을 하구요

15
00:00:48,540 --> 00:00:50,540
'프로파일' 을 선택하죠

16
00:00:50,660 --> 00:00:53,620
보시는건 '테스터' 라는 이름의 프로파일이죠

17
00:00:55,560 --> 00:00:58,620
그리고 다음화면에서는 여러분이 보고 싶은 영화를 고를 수 있도록 합니다

18
00:00:58,620 --> 00:01:01,340
음 그런데 그전에, 여러분들 중 일부는 보셨을지 모르겠지만

19
00:01:01,380 --> 00:01:06,160
영화를 선택하고 나면 '여러분의 개인 취향'을 생성하는 과정이 있습니다

20
00:01:06,800 --> 00:01:09,860
이 과정은 바로 여러분만을 위한 홈페이지를 만드는 과정이라고 생각하면 됩니다

21
00:01:11,060 --> 00:01:14,900
끝나면 선택한 영화의 화면이 나오죠. 지금은 하우스 오브 카드구요

22
00:01:16,740 --> 00:01:19,860
환상적인 다큐멘터리인 '살인자 만들기' 일 수도 있습니다

23
00:01:21,040 --> 00:01:24,940
수백번은 보셨을 '포레스트 검프'를 찾기 위해 '톰 행크스'로 검색을 하실 수도 있구요

24
00:01:26,280 --> 00:01:28,280
'나르코스' 를 선택 하실 수도 있습니다

25
00:01:28,360 --> 00:01:31,380
사진에는 시즌2가 10월 2일에 나온다고 했지만, 이미 출시 되었죠

26
00:01:31,380 --> 00:01:34,660
관심 있으신 분들은 지금 바로 시청하실 수 있습니다

27
00:01:35,220 --> 00:01:37,220
첫번째 에피소드를 선택 하시면

28
00:01:37,360 --> 00:01:41,720
주인공이 자기 이름 '파블로 에밀리오 에스코바 가빌리아' 라는 이름을 소개하면서

29
00:01:41,760 --> 00:01:44,140
플라토 플로모 라는 사람과 대화를 합니다

30
00:01:44,500 --> 00:01:48,800
다 보시고 난 후에 감상이 좋았다면, 5개의 별점을 줄 수도 있습니다

31
00:01:50,960 --> 00:01:54,740
그리고 여러분의 친구들에게 좋아하는 에피소드를 보여줄 수 도 있겠죠

32
00:01:55,100 --> 00:01:57,100
그리고 저희는 이런걸 추적 합니다

33
00:01:58,400 --> 00:02:00,940
여러분이 다른 사이트로 이동하는걸 막기위해

34
00:02:00,940 --> 00:02:04,860
넷플릭스가 여러분의 주의를 환기해야 하는 시간 제한이 얼마일까요

35
00:02:06,180 --> 00:02:07,700
추측해 보세요

36
00:02:08,300 --> 00:02:10,700
네, 어, 생각보다 좀 짧습니다

37
00:02:10,920 --> 00:02:13,260
90초 입니다

38
00:02:13,580 --> 00:02:18,520
그러니까 더 빠르고, 더 부드럽고, 여러분이 좋은 경험을 많이 할 수록,

39
00:02:18,580 --> 00:02:20,480
넷플릭스의 서비스가 그런 품질로 동작해서

40
00:02:20,480 --> 00:02:23,680
그로인해 여러분이 넷플릭스를 계속 시청하도록 하고 싶습니다

41
00:02:24,840 --> 00:02:26,840
이걸 가능하게 하는 방법은

42
00:02:26,860 --> 00:02:29,460
서버쪽에서 이런 저런걸 캐싱 하는 겁니다

43
00:02:29,460 --> 00:02:30,580
그러니까

44
00:02:31,300 --> 00:02:33,780
처음 소개해 드린 일련의 과정을 생각해 보시면

45
00:02:33,820 --> 00:02:35,820
어떤 부분들이 캐시와 관련이 있을까요

46
00:02:36,280 --> 00:02:38,280
회원 가입,

47
00:02:38,800 --> 00:02:40,800
로그인,

48
00:02:41,180 --> 00:02:43,180
프로파일의 선택

49
00:02:43,520 --> 00:02:45,520
좋아하는 비디오의 선택,

50
00:02:45,740 --> 00:02:49,800
뒤에 조금 더 깊이 살펴볼 개인화 부분,

51
00:02:50,820 --> 00:02:52,820
홈페이지를 로딩하는 부분,

52
00:02:52,820 --> 00:02:54,720
홈페이지의 스크롤,

53
00:02:55,140 --> 00:02:58,940
프로파일을 기반으로 한 A/B 테스트,

54
00:02:59,240 --> 00:03:02,580
영화 선택을 위한 이미지 박스까지,

55
00:03:02,660 --> 00:03:05,340
이 외에도 더 많은 것들이 캐시를 사용하구요

56
00:03:05,540 --> 00:03:07,730
게다가 어떤 것들은 하나가 아니라

57
00:03:07,730 --> 00:03:09,920
여러개의 캐시를 사용하기도 합니다

58
00:03:12,020 --> 00:03:14,020
다른 방식으로 보면

59
00:03:14,560 --> 00:03:16,940
이건 일반적인 홈페이지 요청을 보여줍니다

60
00:03:17,100 --> 00:03:19,760
이건 저희 요청 추적 시스템에서 나온 결과물인데요

61
00:03:20,300 --> 00:03:22,560
왼쪽으로 부터 유입된 요청은

62
00:03:22,580 --> 00:03:24,580
오른쪽으로 흘러가고,

63
00:03:24,600 --> 00:03:29,720
오른쪽의 '리프' 노드들의 대부분은 EVCache 입니다

64
00:03:32,120 --> 00:03:34,920
EVCache에 대해 말씀 드렸는데요,

65
00:03:35,200 --> 00:03:37,200
EVCache에 대해 조금 더 살펴보도록 합시다

66
00:03:42,940 --> 00:03:45,500
EVCache는, 짧은 수명의 휘발성 캐시의 약자 입니다

67
00:03:45,520 --> 00:03:47,520
키-밸류 저장소구요

68
00:03:47,740 --> 00:03:50,660
아마존 웹 서비스의 사용을 위해 최적화 되었으며

69
00:03:50,760 --> 00:03:53,280
넷플릭스의 사용성에 맞도록 튜닝 된 서비스 입니다

70
00:03:56,060 --> 00:03:58,094
EVCache는 분산 시스템이고

71
00:03:58,094 --> 00:04:02,760
샤딩과 복제를 단일 리전 또는 멀티리전으로 지원하는 키-밸류 저장소이며

72
00:04:02,880 --> 00:04:04,880
memcached 를 기반으로 만들어 졌고

73
00:04:04,880 --> 00:04:06,700
매우 높은 내구성을 가지고 있는데,

74
00:04:06,780 --> 00:04:09,600
왜냐하면 AWS가 매우 동적인 환경을 제공하기 때문이죠

75
00:04:09,860 --> 00:04:12,200
저의 동료중의 한명이 AWS에 대해 이르기를,

76
00:04:12,340 --> 00:04:14,360
'카오스 몽키 as a 서비스' 라고 하더군요

77
00:04:14,360 --> 00:04:15,500
[폭소]

78
00:04:15,500 --> 00:04:17,000
사실 꽤 맞는 말인데요,

79
00:04:17,000 --> 00:04:18,720
뭐 동료가 한 말이긴 합니다만,

80
00:04:18,860 --> 00:04:22,400
실제로 카오스 몽키보다 AWS가 저희 인스턴스를 더 많이 죽였어요

81
00:04:22,400 --> 00:04:24,260
[다시 폭소]

82
00:04:24,560 --> 00:04:28,560
더 빠른 속도를 위해서 가까운 네트워크를 먼저 사용하도록 하구요

83
00:04:28,580 --> 00:04:33,240
트래픽 증가에 따른 성능 향상 요구를 충족하기 위해 선형적으로 확장 됩니다

84
00:04:33,340 --> 00:04:35,860
내부 고객들을 위해 즉각 배포가 가능하구요

85
00:04:37,000 --> 00:04:39,640
왜 AWS에 최적화 했을까요

86
00:04:39,780 --> 00:04:42,520
말씀드렸듯이, '매우 동적인' 환경인데요, 이게 의미하는 바는

87
00:04:42,620 --> 00:04:45,380
인스턴스는 언제든 사라질 수 있구요,

88
00:04:45,380 --> 00:04:47,060
존(AZ) 전체가 다운될 수도 있구요

89
00:04:47,100 --> 00:04:50,580
다양한 이유에 의해 전체 리전에 불안정해지기도 합니다

90
00:04:51,240 --> 00:04:53,240
네트워크에서 유실도 발생하는 편이구요

91
00:04:54,620 --> 00:04:59,420
동일한 사용자의 세션이 어떤 경우엔 리전별로 서로 바운스되는 경우도 있습니다

92
00:05:00,000 --> 00:05:02,540
인생이 그렇듯이, 실패는 언제나 발생하구요

93
00:05:02,680 --> 00:05:04,535
이에 대해 우리는 항상 준비해야 하며

94
00:05:04,535 --> 00:05:06,760
테스트가 되었는지 확인하고 있습니다

95
00:05:09,340 --> 00:05:13,400
음, 넷플릭스가 사용하는 EVCache의 확장에 대해 말해 보자면

96
00:05:14,840 --> 00:05:17,380
수백 테라바이트의 데이터가 캐시에 저장되구요

97
00:05:17,700 --> 00:05:20,040
하루에 수조회의 오퍼레이션 요청이 있습니다

98
00:05:20,620 --> 00:05:22,620
수백억개의 아이템이 저장되어 있으며

99
00:05:23,200 --> 00:05:25,560
초당 수천만번의 오퍼레이션 요청과

100
00:05:25,940 --> 00:05:30,180
두개의 서로 다른 리전간의 복제 요청이 초당 백만건에 달합니다

101
00:05:30,260 --> 00:05:32,380
잠시 후에 더 자세히 살펴볼 거구요

102
00:05:32,820 --> 00:05:38,280
딱 이정도는 아니지만, 어쨌든 수천대의 EVCache 서버가 존재합니다

103
00:05:39,100 --> 00:05:42,020
클러스터 하나당 수백개의 인스턴스를 사용하구요

104
00:05:42,280 --> 00:05:46,280
하나의 클러스터는 수백개의 마이크로 서비스 클라이언트에 서비스 하고

105
00:05:46,280 --> 00:05:48,200
열개 정도의 특수 목적 클러스터가 존재합니다

106
00:05:48,200 --> 00:05:51,680
이것들은 서로 다른 목적을 위해 튜닝과 셋업된 클러스터구요

107
00:05:52,200 --> 00:05:55,132
3개의 리전에서 동작하는 이런 규모의 서비스를

108
00:05:55,140 --> 00:05:57,360
4명의 엔지니어가 운용합니다

109
00:05:58,220 --> 00:06:00,220
[와우~]

110
00:06:00,460 --> 00:06:05,300
[박수]

111
00:06:05,600 --> 00:06:09,480
그럼 이제 애플리케이션 서버 하나가

112
00:06:09,480 --> 00:06:12,220
EVCache와 어떻게 통신하는지

113
00:06:12,220 --> 00:06:14,040
어떻게 설정 되는지 살펴 보죠

114
00:06:14,140 --> 00:06:17,020
클라우드 어딘가에 동작하는 애플리케이션 서버가 있구요

115
00:06:17,840 --> 00:06:22,660
다른 서비스에 접근하기 위한 클라이언트 라이브러리가 있습니다

116
00:06:23,180 --> 00:06:27,420
그 라이브러리 안에는 다시 EVCache 라이브러리가 있습니다

117
00:06:27,480 --> 00:06:31,260
자바 클라이언트 라이브러리고, JAR로 제공됩니다

118
00:06:33,100 --> 00:06:35,740
서버 쪽은 먼저 memcached가 있구요

119
00:06:35,820 --> 00:06:38,860
음, 아니면 memcached 와 비슷하다고도 할 수 있겠네요

120
00:06:39,120 --> 00:06:42,440
그리고 서버에는 Java로 구현된 "Prana" 라고 불리는  '사이드카'가 있는데

121
00:06:42,440 --> 00:06:45,120
이건 Java 외의 언어로 만들어진 서비스에도

122
00:06:45,120 --> 00:06:48,560
이건 넷플릭스 플랫폼에서 제공하는 다양한 서비스, 예를 들면

123
00:06:48,560 --> 00:06:53,160
서비스 등록을 위한 '서비스 디스커버리' 또는 
매트릭 서비스 후킹을 제공합니다

124
00:06:53,960 --> 00:06:55,497
애플리케이션의 클라이언트는

125
00:06:55,497 --> 00:06:58,460
TCP를 통해 직접 memcached에 접근하구요

126
00:06:59,020 --> 00:07:02,160
이 '사이드카'에서는 EVCache 서버를 유레카에 등록하면

127
00:07:02,160 --> 00:07:05,400
애플리케이션에서는 이 유레카 정보를 통해 EVCache에 접속합니다

128
00:07:09,340 --> 00:07:13,220
보시는 다이어그램에서, 하나의 클라이언트는 전체 클러스터에 접근이 가능한데,

129
00:07:13,220 --> 00:07:15,820
보시다 시피 3개의 AZ를 자세히 보시면

130
00:07:15,820 --> 00:07:19,420
점선으로 표시된 박스가 있는데, 이것들은 전체 데이터 복제를 나타냅니다

131
00:07:19,500 --> 00:07:22,960
하나의 데이터가 AZ 별로 하나씩, 3개의 복제를 가지는 구성입니다

132
00:07:23,080 --> 00:07:26,500
그리고 클라이언트는 이 모든 EVCache에 접근 가능합니다

133
00:07:26,500 --> 00:07:30,600
클라이언트들은 다시 모든 캐시에 접근이 가능합니다

134
00:07:30,860 --> 00:07:33,920
음, 이건 저희가 'bipartite' 라고 부르는 아키텍처입니다

135
00:07:33,920 --> 00:07:35,760
모든 클라이언트가 모든 서버에 접근 가능하고

136
00:07:35,760 --> 00:07:38,940
, 서버간에는 서로 연결하지 않는 구조입니다.

137
00:07:40,200 --> 00:07:42,200
데이터 읽기의 경우

138
00:07:42,700 --> 00:07:45,380
굉장히 단순한데요, 가장 가까이 있는 서버에 먼저 접근합니다

139
00:07:46,820 --> 00:07:48,480
이게 동작하지 않는 경우를 대비해

140
00:07:48,480 --> 00:07:50,260
몇가지 백업 경로가 존재하구요

141
00:07:50,260 --> 00:07:53,100
인스턴스가 갑자기 죽거나, 네트워크 지연시간이 상승하거나 하는

142
00:07:53,100 --> 00:07:55,780
이런 예측 불가능한 장애를 대비하기 위함입니다

143
00:07:55,820 --> 00:07:57,820
이 경우 다른 노드에 접근을 시도합니다

144
00:07:59,720 --> 00:08:01,720
데이터 쓰기의 경우 조금 더 흥미로운 부분이 있는데요

145
00:08:01,980 --> 00:08:04,480
클라이언트는 지정된 3개의 모든 노드에 쓰기를 시도합니다

146
00:08:04,600 --> 00:08:07,080
이게 동일한 데이터에 대해 여러개의 복제본을 가지는 방법입니다

147
00:08:07,940 --> 00:08:11,440
모든 클라이언트는 서로 다른 여러 지역에 데이터를 저장합니다

148
00:08:16,660 --> 00:08:19,300
이제 몇가지 사례를 살펴볼텐데요

149
00:08:19,360 --> 00:08:22,000
아주 단순한 부분부터 복잡한 부분까지를 포함합니다

150
00:08:24,760 --> 00:08:27,240
이런 방법은 매우 직접적이고 일반적이라 할 수 있는데

151
00:08:27,340 --> 00:08:29,340
엄청 느린 서비스에 엄청 빠른 캐시가 존재한다면

152
00:08:29,380 --> 00:08:31,940
당연히 캐시를 먼저 참조하도록 할겁니다

153
00:08:31,940 --> 00:08:33,780
캐시 참조가 불가능한 경우 서비스를 참조하겠죠

154
00:08:33,920 --> 00:08:37,540
보시는 그림에서 클라이언트 라이브러리는 EVCache 에 먼저 접근합니다

155
00:08:38,140 --> 00:08:41,480
만약 여기에 원하는 데이터가 없다면 데이터베이스에 접근합니다

156
00:08:41,480 --> 00:08:43,320
저희의 경우 이 서비스는 '카산드라'입니다

157
00:08:43,480 --> 00:08:46,660
그래서 데이터를 찾으면 서비스는 캐시에 해당 데이터를 저장합니다

158
00:08:46,880 --> 00:08:49,600
데이터를 리턴하기 전에 또는 리턴 후 비동기 방식으로요

159
00:08:52,000 --> 00:08:53,200
두번째로는

160
00:08:53,340 --> 00:08:54,880
단기적 데이터 저장소로서의 사용 사례입니다

161
00:08:54,880 --> 00:08:57,900
예를 들면, 음. 넷플릭스 시청시 발생하는 '재생' 세션 같은 데이터 입니다

162
00:08:57,900 --> 00:09:00,400
여러분이 영상을 어떻게 재생하고 있는지 정보를 수집합니다

163
00:09:00,840 --> 00:09:03,980
시간의 흐름에 따라 다양한 애플리케이션들이 캐시를 사용합니다

164
00:09:04,080 --> 00:09:06,080
어떤 애플리케이션은 세션을 시작하면서 접근하고,

165
00:09:06,100 --> 00:09:08,840
어떤건 세션을 업데이트 하거나 종료하는 등의 용도로 접근합니다

166
00:09:09,360 --> 00:09:12,800
그래서 지금 보시는 그림에는 데이터베이스 없이 캐시만 존재합니다

167
00:09:15,660 --> 00:09:18,240
음 이건 저에겐 꽤 흥미로운 사용사례인데요

168
00:09:18,300 --> 00:09:21,920
이 모델은 저희가 가장 많이 사용하는 형태인데

169
00:09:22,100 --> 00:09:25,780
데이터에 대한 '주 저장소' 메커니즘 입니다

170
00:09:26,080 --> 00:09:30,360
넷플릭스는 밤마다 거대한 프로세스를 돌리는데

171
00:09:30,360 --> 00:09:33,900
모든 사용자의 프로파일을 바탕으로 모든 사용자의 페이지를 새로 만드는 작업 같은것입니다

172
00:09:34,600 --> 00:09:37,420
엄청나게 많은 컴퓨팅 자원과 엄청나게 많은 데이터가 발생합니다

173
00:09:37,420 --> 00:09:40,720
그리고 이런 연산의 결과로 생성된 데이터는 EVCache에 저장됩니다

174
00:09:41,900 --> 00:09:44,340
그러면 온라인 서비스들은 이 데이터를 읽게되죠

175
00:09:44,380 --> 00:09:47,100
언제 무슨 데이터가 쓰여져있는지는 관계 없는 구조입니다

176
00:09:47,260 --> 00:09:50,260
넷플릭스는 문제에 대한 대비를 확실하게 하는 문화가 있는데

177
00:09:50,280 --> 00:09:53,300
원하는 데이터가 항상 즉시 존재하지 않아도 괜찮습니다

178
00:09:54,200 --> 00:09:56,200
전체 그림은 이런 모양이 되는데

179
00:09:56,200 --> 00:09:57,560
오프라인 서비스들이 캐시에 데이터를 업데이트하고

180
00:09:57,740 --> 00:09:59,740
온라인 서비스는 이 캐시에 접근해 데이터를 읽습니다

181
00:09:59,740 --> 00:10:01,700
이 경우 캐시는 마치 온라인과 오프라인 서비스 사이의 버퍼처럼 동작합니다

182
00:10:01,760 --> 00:10:04,560
또는 게이트웨이라던가, 아무튼 그런 동작을 합니다

183
00:10:05,920 --> 00:10:08,640
음 이건 넷플릭스의 '개인화 데이터' 제공에 사용됩니다

184
00:10:10,180 --> 00:10:12,180
이건 더 흥미로운데요

185
00:10:12,200 --> 00:10:14,740
엄청나게 많은 양의 데이터가

186
00:10:15,060 --> 00:10:17,200
매우 높은 가용성을 지녀야 하기 때문입니다

187
00:10:17,200 --> 00:10:19,120
예를 들면 UI 문자열 같은것들

188
00:10:19,720 --> 00:10:21,700
만약 이 데이터가 없다면

189
00:10:21,700 --> 00:10:24,780
페이지에 아무 글자도 표시되지 않기 때문에 사용성이 매우 나빠지겠죠

190
00:10:25,380 --> 00:10:28,040
이런 데이터들은 인메모리 캐시에 저장되는데

191
00:10:28,400 --> 00:10:30,600
여기에 데이터가 존재하지 않는 경우에는

192
00:10:30,680 --> 00:10:33,200
백그라운드 프로세스를 통해 EVCache 로부터 데이터를 받아 갱신할 수도 있습니다

193
00:10:33,800 --> 00:10:37,160
따라서 데이터가 메모리에 없는 경우에는 EVCache에 접근해서

194
00:10:37,260 --> 00:10:39,720
데이터를 가져오면 백그라운드에서 갱신을 하는 형태입니다

195
00:10:40,440 --> 00:10:42,740
이런 경우를 생각해 보세요

196
00:10:42,900 --> 00:10:45,260
하루 중 피크 시간대에 사용자가 몰리면

197
00:10:45,320 --> 00:10:48,780
스케일 아웃을 통해 서버를 새로 만들고 이들은 EVCache 에서 데이터를 가져갑니다

198
00:10:48,780 --> 00:10:50,640
그리고 여기에 더해 또 다른 프로세스가 있는데

199
00:10:50,640 --> 00:10:52,420
최신 영화 패키지 같은 데이터들이

200
00:10:52,460 --> 00:10:54,960
비동기로 캐시에 업데이트되는 것입니다

201
00:10:55,740 --> 00:10:57,740
그리고 이 너머에는

202
00:10:57,800 --> 00:11:00,760
애플리케이션은 데이터를 가지고 있는 데이터베이스에 직접

203
00:11:00,760 --> 00:11:02,020
접근할 수도 있지만

204
00:11:02,020 --> 00:11:04,420
대부분의 경우 이부분은 '옵션(차선)'으로 추가 됩니다

205
00:11:04,540 --> 00:11:08,440
EVCache의 성능과 업타임이 너무 좋아서 데이터베이스를 굳이 고려하지 않거든요

206
00:11:08,940 --> 00:11:10,940
[일부 청중 웃음]

207
00:11:11,840 --> 00:11:14,680
지금까지 이런 캐시 동작을 하이레벨에서 살펴보았는데요

208
00:11:14,680 --> 00:11:17,840
좀 더 자세하게 살펴봅시다

209
00:11:19,180 --> 00:11:22,560
제가 '개인화 파이프라인이'라고 부르는 부분을 함께 보시죠

210
00:11:22,760 --> 00:11:24,740
넷플릭스의 개인 사용자 홈페이지를 새로 만든다고 합시다

211
00:11:24,740 --> 00:11:27,240
이건 하나의 단순한 과정이 아닌, 매우 많은 단계를 거쳐 완성됩니다

212
00:11:27,360 --> 00:11:29,780
서로 다른 알고리즘이 서로 다른 일을 처리합니다

213
00:11:30,300 --> 00:11:32,720
어떤 시점에 'A'를 처리할거구요

214
00:11:33,380 --> 00:11:36,060
이 결과를 EVCache 에 퍼블리싱 합니다

215
00:11:36,180 --> 00:11:40,080
이 '컴퓨트 A'에 필요한 원본 데이터 저장소는 오프라인에 위치한 '하이브' 일수도,

216
00:11:40,240 --> 00:11:42,420
또는 다른 데이터 저장소 일겁니다

217
00:11:43,000 --> 00:11:45,000
그리고 나서 '컴퓨트 B' 작업을 시작합니다

218
00:11:45,400 --> 00:11:47,700
그리고 A와 B의 결과를 참조하는 C가 있을 수 있구요

219
00:11:47,700 --> 00:11:48,860
'D' 가 있을 수도

220
00:11:48,980 --> 00:11:52,140
'E'가 있을 수도 있으며, 이것들은 서로 다른 데이터 의존성을 가집니다

221
00:11:52,400 --> 00:11:54,400
개인화 과정에서 말이죠

222
00:11:54,400 --> 00:11:56,240
이 작업들은 모두 '오프라인' 서비스에서 처리됩니다

223
00:11:56,640 --> 00:12:00,280
온라인 서비스들인 단순히 최종 결과물을 가져다 쓰는게 아닙니다

224
00:12:00,420 --> 00:12:02,720
서비스에 필요한 결과물의 서로 다른 부분을 참조할 수도 있습니다

225
00:12:02,780 --> 00:12:04,780
서비스별로 서로 다른 사용자 경험을 제공하기 위해서죠

226
00:12:04,780 --> 00:12:05,780
'온라인 1'의 경우

227
00:12:05,780 --> 00:12:07,240
A, B, C, D 를 참조하고

228
00:12:07,240 --> 00:12:09,520
'온라인 2'는 이와 다른 세트를 참조합니다

229
00:12:14,740 --> 00:12:17,920
'폴리글럿' 역시 넷플릭스에서 흥미로운 주제입니다 
(필요에 따라 서로 다른 기술 스택을 선택하는 것)

230
00:12:17,920 --> 00:12:20,140
바로 오늘 현재까지도 뜨거운 감자에요

231
00:12:20,360 --> 00:12:23,620
하지만 넷플릭스 서비스의 중요한 부분을 담당하고 있는 저희 팀은

232
00:12:23,620 --> 00:12:25,980
저희만의 해결 방법을 적용하기로 결정했는데요

233
00:12:26,360 --> 00:12:28,760
예나 지금이나

234
00:12:28,820 --> 00:12:32,780
보신바와 같이 자바로 구현되어 있습니다

235
00:12:34,280 --> 00:12:38,120
하지만 다른 팀들은 필요에 따라 '사이드카'를 사용합니다.
(넷플릭스 구조에서 사용되는 다른 클라이언트 도구들)

236
00:12:39,460 --> 00:12:42,460
그래서 저희는 앱 로컬 머신의 사이드카를 통해 HTTP REST API를 제공하구요

237
00:12:42,460 --> 00:12:46,020
다른 팀의 개발자들이 캐시에 HTTP를 통해 접근할 수 있도록 말이죠

238
00:12:47,180 --> 00:12:49,180
이걸 로컬에서 분리해서 '원격' 모델을 사용해 접근하는 것도 가능합니다

239
00:12:49,940 --> 00:12:52,340
그래서 원격에서 HTTP API로 접근할 수도 있습니다

240
00:12:52,880 --> 00:12:55,500
추가로 현재 실헙하고 있는 memcached 의 프로토콜 역시

241
00:12:55,500 --> 00:12:57,260
현재 구현중에 있습니다

242
00:12:57,360 --> 00:12:59,840
내부의 다른 파트너 팀과 함께 작업중입니다

243
00:13:00,320 --> 00:13:03,440
현재 활발하게 개발 중인 부분이구요

244
00:13:03,480 --> 00:13:05,860
그래서 현재로서는 자세하게 알고 있지는 않습니다

245
00:13:06,500 --> 00:13:10,940
궁금하신 사항이 있다면 세션이 끝나고 저희 부스로 오세요

246
00:13:14,320 --> 00:13:18,140
지금까지 보여드린것 외에 추가되는 내용들이 있습니다

247
00:13:18,220 --> 00:13:21,260
이 추가 기능들은 넷플릭스가 확장 가능하도록 지원합니다

248
00:13:21,840 --> 00:13:25,540
첫번째는 '크로스 리전' 글로벌 데이터 복제 입니다

249
00:13:26,420 --> 00:13:29,300
더 빠른 배포를 위해 '캐시 워밍' 기능을 제공하구요

250
00:13:29,680 --> 00:13:31,840
세컨드리 인덱싱 역시 지원하고 있습니다

251
00:13:32,080 --> 00:13:34,260
디버깅을 위한 포인트 쿼리

252
00:13:34,760 --> 00:13:40,040
캐시 데이터의 일관성을 확인하기 위한 매트릭스 역시 제공합니다

253
00:13:40,200 --> 00:13:45,280
이 모든 기능들은 kafka 클러스터를 통해 전송되는 '메타데이터' 기반으로 동작합니다

254
00:13:48,060 --> 00:13:51,800
크로스 리전 복제에 대해 살펴봅시다

255
00:13:52,180 --> 00:13:54,180
캐시 워밍도요

256
00:13:54,540 --> 00:13:57,700
첫번째는 복제 부분입니다

257
00:13:58,700 --> 00:14:01,280
여기 두개의 리전이 있습니다. 실제로는 3개지만요

258
00:14:01,340 --> 00:14:05,380
3개가 서로 페어로 동작하지만, 세션에서는 2개를 통해 설명할게요

259
00:14:06,300 --> 00:14:08,720
리전 A의 앱과 리전 B의 앱

260
00:14:08,720 --> 00:14:11,960
그리고 이들 리전에 매칭되는 캐시가 리전 A, B에 각각 존재합니다

261
00:14:12,520 --> 00:14:15,460
리전 A의 앱이 리전 A의 캐시에 데이터를 추가하면

262
00:14:15,960 --> 00:14:19,900
이 새로운 데이터에 대한 메타데이터 정보가 카프카로 업데이트 됩니다

263
00:14:21,300 --> 00:14:23,400
'복제 릴레이'로 불리는 앱이

264
00:14:23,400 --> 00:14:25,400
카프카에 올라온 메타데이터를 꺼내오구요

265
00:14:25,920 --> 00:14:28,560
만약 데이터가 필요한 경우라면 캐시로 부터 해당 데이터를 가져옵니다

266
00:14:28,780 --> 00:14:31,540
그리고 이 데이터를 다른 리전으로의 링크로 전송합니다

267
00:14:31,820 --> 00:14:33,820
'복제 프락시'로 불리는 앱이 이 데이터를 받으면

268
00:14:33,820 --> 00:14:38,300
리전 A의 앱이 했던 것처럼, 리전 B의 캐시에 데이터를 추가합니다

269
00:14:38,420 --> 00:14:41,340
그러면 리전 B의 애플리케이션은 리전 B의 업데이트된 캐시 데이터를 읽을 수 있습니다

270
00:14:41,520 --> 00:14:43,520
이 동작은 양 방향에서 동작합니다

271
00:14:43,580 --> 00:14:46,680
그리고 추가되는 리전간에도 동일하게 동작하지요

272
00:14:49,760 --> 00:14:51,160
이번에는 캐시 워밍입니다

273
00:14:51,160 --> 00:14:52,240
현재는 안정된 상태로 동작중인 그림입니다

274
00:14:52,240 --> 00:14:55,360
지금 보시는 그림은 이미 보셨던 건데요, 거기엔 카프카가 없었죠

275
00:14:55,360 --> 00:14:59,020
현재 2개로 안정적인 클러스터를, 두배로 확장한다면 어떨까요

276
00:14:59,040 --> 00:15:01,020
지금은 2개노드 클러스터지만 4대 노드 클러스터로 확장하고 싶다고 합시다

277
00:15:02,000 --> 00:15:04,760
두번째 '새로운' 캐시 클러스터는 4개 노드로 생성하고

278
00:15:04,760 --> 00:15:06,620
애플리케이션은 이제 이 새로운 클러스터로 접근해야 합니다

279
00:15:07,420 --> 00:15:11,160
'캐시 워머'로 불리는 앱은, 카프카로부터 메타데이터 스트림을 받아서

280
00:15:11,400 --> 00:15:14,320
기존의 캐시에 있던 데이터를 받아다가 새로운 캐시 클러스터에 동기화 합니다

281
00:15:14,320 --> 00:15:16,320
이렇게 새로 생긴 캐시는 기존의 캐시와 동일한 데이터를 가집니다

282
00:15:16,640 --> 00:15:20,100
이제 '캐시 워머'를 지우고, '오래된' 클러스터를 제거합니다

283
00:15:20,480 --> 00:15:22,860
아, 먼저 '읽기' 요청을 새로운 4개의 클러스터로 보내야죠

284
00:15:22,860 --> 00:15:24,500
그러고 나서 이전 2개 클러스터를 제거합니다

285
00:15:24,980 --> 00:15:26,980
그럼 이제 새롭게 안정된 클러스터를 확보할 수 있습니다

286
00:15:27,380 --> 00:15:30,200
이제 이런 동작은 memcached에 새롭게 추가된 기능을 사용해서

287
00:15:30,260 --> 00:15:32,560
동일한 기능을 하는 부분을 구현중입니다

288
00:15:32,740 --> 00:15:35,140
그러면 여기 있는 카프카는 사용하지 않아도 됩니다

289
00:15:35,200 --> 00:15:38,440
현재로서는 이 새로운 기능을 프로덕션에 어떻게 반영할지 고려하고 있습니다

290
00:15:40,560 --> 00:15:43,280
보시는 내용이 클라이언트가 사용하는 코드의 전부 입니다

291
00:15:43,340 --> 00:15:47,740
지금까지 보여드린 모든 기능을 사용하는 코드죠

292
00:15:48,140 --> 00:15:51,940
memcached를 사용할때 일반적으로 사용하는 SET, GET, REPLACE 같은

293
00:15:51,940 --> 00:15:55,020
명령어를 클라이언트를 설정한 후 사용하면 됩니다

294
00:15:55,820 --> 00:15:58,580
우리 서비스의 모든 EVCache 클라이언트가 내부적으로 사용하는 방법입니다

295
00:16:03,360 --> 00:16:05,360
음, 이제 내용을 좀 바꿔보도록 하겠습니다

296
00:16:05,360 --> 00:16:07,020
그동안 제가 일해왔던,

297
00:16:07,180 --> 00:16:09,180
음, 제가 지금까지 보여드렸던

298
00:16:10,240 --> 00:16:13,720
이 모든 구조적인 디자인들은 모두

299
00:16:13,800 --> 00:16:16,920
대단위 환경에서 동작하는 것들입니다

300
00:16:18,640 --> 00:16:21,240
서버 측면에서 조금 더 깊이 들어가 보도록 하겠습니다

301
00:16:22,120 --> 00:16:24,880
이건 우리의 새로운 버전의 캐시 서버입니다

302
00:16:24,940 --> 00:16:28,060
곧 서비스에 적용될 예정인데요

303
00:16:28,060 --> 00:16:29,860
요구사항들이 반영된

304
00:16:29,860 --> 00:16:31,860
이 프로젝트의 이름은 '모네타'입니다

305
00:16:33,200 --> 00:16:35,200
'모네타'라는 이름은

306
00:16:35,200 --> 00:16:37,380
'기억'의 여신의 이름입니다

307
00:16:37,420 --> 00:16:39,760
주노 펀드의 보호자의 이름을 따 온 것이기도 합니다
(주노: 그리스 신화의 헤라)

308
00:16:40,160 --> 00:16:43,340
이건 '디스크'에 데이터를 저장하는 혁신적인 서버 입니다

309
00:16:44,620 --> 00:16:47,040
비용을 절감하기 위한 내부 프로젝트입니다

310
00:16:48,020 --> 00:16:50,860
실제로 이 모네타는 비용을 절감하고 있는데

311
00:16:50,940 --> 00:16:55,900
현재의 비용 뿐만 아니라 미래에 필요할 것으로 예상되는 메모리에 대한 비용 역시 절감하고 있습니다

312
00:16:56,420 --> 00:16:58,260
이건 저희에게 매우 중요한 부분입니다

313
00:16:58,260 --> 00:17:00,680
우린 스트림이 시작될때부터 발생하는 비용을 추적하고 있는데

314
00:17:00,680 --> 00:17:02,340
파이낸스팀 역시 이 지표를 보고 있습니다

315
00:17:02,560 --> 00:17:04,628
디스크를 사용하는 모델로의 변경이

316
00:17:04,628 --> 00:17:07,960
미래에 발생할 비용구조 개선에 연관이 있다고 생각합니다

317
00:17:08,500 --> 00:17:13,140
여러개의 리전간에 발생하는 요청 부분에도 개선이 있을것으로 기대합니다

318
00:17:15,140 --> 00:17:17,140
이미 보셨듯이 우리가 사용했던 이전 서버의 모델은

319
00:17:17,140 --> 00:17:18,820
memcached 와 Prana 로 되어 있습니다

320
00:17:18,820 --> 00:17:20,520
굉장한 구조라고 보긴 힘들죠

321
00:17:21,220 --> 00:17:23,220
모든 데이터는 메모리(RAM)에 저장됩니다

322
00:17:23,320 --> 00:17:26,720
사실 이건 디스크와 비교하면 꽤나 비싼 자원입니다

323
00:17:27,780 --> 00:17:31,540
지난해 넷플릭스는 아키텍처를 변경했습니다

324
00:17:31,540 --> 00:17:33,120
'우리가' 아키텍처를 변경한거죠

325
00:17:33,560 --> 00:17:37,400
바로 기존 모델에서 리전을 하나 더 사용하는 N+1 구조로 변경한거죠

326
00:17:37,600 --> 00:17:40,660
어느 하나의 리전에서 장애가 발생한다면

327
00:17:40,660 --> 00:17:43,500
이 리전의 트래픽을 다른 2개의 리전으로 전환해서 극복할 수 있습니다

328
00:17:43,680 --> 00:17:46,660
따라서 모든 서비스 멤버들은 각각의 리전에 존재해야 합니다

329
00:17:49,260 --> 00:17:52,020
추가로 말씀드리자면, 올해 첫달부터

330
00:17:52,220 --> 00:17:55,020
130개의 국가에 넷플릭스 서비스를 시작했습니다

331
00:17:55,260 --> 00:17:57,560
기존보다 훨씬 많은 데이터가 발생하는거죠

332
00:17:58,220 --> 00:18:00,220
최적화가 필요했습니다

333
00:18:00,860 --> 00:18:04,820
이전에 말씀드렸던 개인화를 위한 컴퓨팅을 기억하실 겁니다

334
00:18:05,800 --> 00:18:07,800
글로벌 데이터란 사실 많은 복제본을 의미합니다

335
00:18:08,060 --> 00:18:10,480
그림에서 보셨던 3개의 복제는

336
00:18:10,580 --> 00:18:12,580
실제로 글로벌에서는 9개가 됩니다

337
00:18:12,640 --> 00:18:15,980
우리가 3개의 리전을 운용하고 있기 때문에 그렇습니다

338
00:18:16,700 --> 00:18:19,660
하지만 우리의 캐시 접근 패턴은 매우 '리전 종속적' 입니다

339
00:18:19,660 --> 00:18:22,580
여러분이 만약 캘리포니아에서 넷플릭스를 보신다면

340
00:18:22,600 --> 00:18:24,940
여러분은 넷플릭스의 us-west-2 리전을 사용하고 있는겁니다

341
00:18:25,760 --> 00:18:29,960
이 경우, 여러분의 트래픽이 유럽이나 미국 동부에서

342
00:18:29,960 --> 00:18:33,220
발생할 확율은 매우 낮습니다

343
00:18:33,560 --> 00:18:37,820
따라서 특정 리전은 매우 자주 사용되는 반면, 나머지 리전은 매우 사용되지 않는 상태가 됩니다

344
00:18:39,820 --> 00:18:42,680
그래서 자주 사용되는건 메모리에, 아닌것들은 디스크에 저장하는거죠

345
00:18:43,880 --> 00:18:49,040
따라서 실제 사용되는 데이터는 메모리에, 전체 데이터는 디스크에 보관됩니다

346
00:18:50,100 --> 00:18:52,100
그래서 이 '새로운' 서버의 모습은,

347
00:18:52,580 --> 00:18:54,580
몇가지 새로운 프로세스를 사용합니다

348
00:18:54,580 --> 00:18:56,200
하나는 'Rend' 구요

349
00:18:56,200 --> 00:18:57,940
다른 하나는 'Mnemonic' 입니다

350
00:18:57,940 --> 00:18:59,480
그리고 memcached 역시 사용합니다

351
00:19:00,220 --> 00:19:04,200
이 구조는 L1과 L2 캐시처럼 동작합니다

352
00:19:04,400 --> 00:19:07,460
애플리케이션이 데이터를 필요로 할때는 언제든

353
00:19:07,480 --> 00:19:09,560
정적 데이터 분리같은 것들을

354
00:19:09,560 --> 00:19:11,780
분산을 고려하지 않고도 언제든 데이터를 사용할 수 있습니다

355
00:19:12,900 --> 00:19:16,260
그리고 이 세개의 프로세스는 모두 동일한 프로토콜을 사용합니다

356
00:19:16,260 --> 00:19:17,480
따라서 동일한 디버그 도구의 사용이 가능하고,

357
00:19:17,480 --> 00:19:19,260
동일한 부하 테스트 도구를 사용해서

358
00:19:19,260 --> 00:19:20,860
각각의 서버들을 테스트할 수 있습니다

359
00:19:23,420 --> 00:19:25,840
여기 보시는 'Rend'가 하는 일은 '프락시' 입니다

360
00:19:25,840 --> 00:19:27,580
외부의 연결 요청을 받죠

361
00:19:27,680 --> 00:19:31,100
그리고 내부의 memcached 및 Mnemonic과 연결을 유지합니다

362
00:19:31,200 --> 00:19:33,300
우리의 L1과 L2 캐시죠

363
00:19:34,780 --> 00:19:36,780
'Rend' 를 먼저 살펴보고

364
00:19:36,780 --> 00:19:38,060
Mnemonic 을 그 다음에 보겠습니다

365
00:19:38,060 --> 00:19:39,780
그리고 모든것들을 함께 보구요

366
00:19:39,840 --> 00:19:42,160
이것들이 가져다 주는 성능 수치도 보구요

367
00:19:42,300 --> 00:19:44,800
제게는 마지막 부분이 가장 즐거울것 같네요

368
00:19:45,420 --> 00:19:47,680
혹시 여기 계신분들 중 Go 개발자 계신가요?

369
00:19:48,400 --> 00:19:49,520
네 있네요

370
00:19:49,520 --> 00:19:51,020
이 프로젝트는 Go로 되어있습니다. 가서 확인해 보세요

371
00:19:51,060 --> 00:19:53,060
공개된 프로젝트입니다

372
00:19:53,120 --> 00:19:55,540
Github의 넷플릭스 repo에 있구요

373
00:19:55,540 --> 00:19:56,740
암

374
00:19:57,060 --> 00:19:59,060
즐거운 일이죠?

375
00:19:59,580 --> 00:20:03,340
이건 기본적으로 고성능의 memcached 프락시 서버입니다

376
00:20:03,360 --> 00:20:05,740
클라이언트에 변경이 필요하지 않습니다

377
00:20:06,300 --> 00:20:09,740
높은 수준의 동시 접근성을 위해 Go로 개발되었구요

378
00:20:09,920 --> 00:20:15,500
개발자들이 새로운 기능을 하는 코드를 작성하기 매우 쉬울 뿐만 아니라

379
00:20:15,820 --> 00:20:19,020
프로덕션에서도 매우 빠른 성능을 기대할 수 있기 때문입니다

380
00:20:19,460 --> 00:20:23,360
이 Rend 의 목적은 단일 머신내에서 L1/L2 캐시 관리입니다

381
00:20:24,240 --> 00:20:26,540
이 프로젝트가 시작되기 전부터 이 캐시는

382
00:20:26,540 --> 00:20:30,200
읽기 및 쓰기 동작을 위해 수만개의 동시 접속 처리 요구사항이 있었고

383
00:20:30,320 --> 00:20:33,020
이 요구사항은 프로젝트의 시작부터 존재했던 것입니다

384
00:20:35,400 --> 00:20:37,820
Rend의 내부 구조를 더 자세하게 살펴보자면

385
00:20:37,820 --> 00:20:39,640
모듈화된 소프트웨어 프로젝트임을 알 수 있습니다

386
00:20:39,760 --> 00:20:43,560
깃헙에 올라가 있는 프로젝트가 실제 넷플릭스의 프로덕션에 사용되고 있습니다

387
00:20:44,040 --> 00:20:48,760
이건 외부 요청을 받고 내부의 L1이나 L2로의 접근을 판단합니다

388
00:20:48,840 --> 00:20:51,100
또는 다른 백엔드 서버와 통신하기도 하는데

389
00:20:51,120 --> 00:20:54,060
예를들면 넷플릭스가 만든 메트릭스 라이브러리와 같은 백엔드 입니다

390
00:20:54,060 --> 00:20:55,920
아, 그렇지만

391
00:20:56,040 --> 00:20:58,700
Go 세상에는 우리가 원했던 것이 없었기 때문에

392
00:20:58,700 --> 00:21:01,380
왜냐하면 넷플릭스가 만든 아틀라스 같은 시스템이 있었기 때문이죠

393
00:21:01,800 --> 00:21:03,800
다수의 '오케스트레이터'를 가지고 있는데요

394
00:21:03,900 --> 00:21:07,620
모든것들을 함께 두고 보았을때 확실해 지는 것들이 있기 때문입니다

395
00:21:08,300 --> 00:21:10,840
그리고 '패러랠 락'이라는 기능이 있는데

396
00:21:10,920 --> 00:21:15,820
어떤 한대의 서버가 특정 시점에 엄청나게 많은 요청을 받는 상황이 있을 수 있습니다

397
00:21:16,020 --> 00:21:20,300
하지만 어떤 경우에도 하나의 키에 두개의 값으로 동시에 수정하는 것은 불가능하죠

398
00:21:20,420 --> 00:21:24,900
그래서 이 패러랠 락은 캐시안에 데이터의 무결성과 L1과 L2의 일관성을 보장하는 역할을 합니다

399
00:21:27,520 --> 00:21:30,100
다음은 L2 부분인 '네모닉' 입니다

400
00:21:31,040 --> 00:21:33,040
'곧' 오픈 소스가 될거라고 말하고 싶습니다

401
00:21:33,100 --> 00:21:36,080
오픈소스화 하는것이 저의 목표는 아니기 때문이죠

402
00:21:36,280 --> 00:21:38,280
하지만 오픈소스화 하고 싶습니다.

403
00:21:40,040 --> 00:21:43,040
이건 기본적으로 SSD 기반의 스토리지 입니다

404
00:21:43,440 --> 00:21:49,360
커넥션 관리자를 통해 rend로 부터 받은 memcached 프로토콜 기반 데이터를 파싱하는데

405
00:21:49,560 --> 00:21:54,240
주된 목표는 memcached 동작을 rocksDB에 매핑하는 것입니다

406
00:21:54,440 --> 00:21:56,680
그림의 제일 아래에 보시면 RocksDB가 있구요

407
00:21:56,680 --> 00:21:59,220
외부 연결은 그림에서 위로부터 아래로 진행됩니다

408
00:21:59,220 --> 00:22:01,600
제일 상단의 Rend 를 통해 요청이 유입되고

409
00:22:01,960 --> 00:22:05,440
이 요청은 중간의 C++ 로 되어있는

410
00:22:06,320 --> 00:22:11,980
RocksDB의 라이브러리를 사용하는 니모닉 코어를 통해 디스크에 저장됩니다

411
00:22:13,920 --> 00:22:15,920
우리는 몇가지 이유로 인해 RocksDB를 선택했습니다

412
00:22:15,960 --> 00:22:17,960
가장 주된 이유는

413
00:22:17,960 --> 00:22:19,720
RocksDB에 데이터를 쓸때

414
00:22:19,740 --> 00:22:21,920
RocksDB는 인메모리 버퍼에 이 데이터를 바로 저장하고

415
00:22:22,000 --> 00:22:25,200
그 후에 비동기 동작을 통해 디스크에 파일로 저장합니다

416
00:22:25,420 --> 00:22:28,060
따라서 매우 빠른 읽기/쓰기 성능을 제공하기 때문입니다

417
00:22:31,520 --> 00:22:33,520
실제로 RockDB를 어떻게 사용할까요

418
00:22:33,740 --> 00:22:37,480
'FIFO Compaction' 이라는 기법을 사용하는데

419
00:22:37,620 --> 00:22:43,040
이건 파일들을 FIFO 큐에 리니어하게 넣어서 디스크에 저장합니다

420
00:22:43,540 --> 00:22:45,540
시간의 흐름에 따라서요

421
00:22:45,540 --> 00:22:47,540
따라서 가장 최신의 파일이 처음 큐에서 나오게 되고

422
00:22:47,660 --> 00:22:49,980
오래된 파일은 큐에서 바로 삭제되는 구조입니다

423
00:22:50,780 --> 00:22:53,760
또한 블룸 필터와 인덱스를 메모리의 특정 위치에 고정하는 기법도 사용하는데

424
00:22:53,940 --> 00:22:57,300
빠른 접근 또는 빠른 캐시미스를 제공합니다

425
00:22:57,560 --> 00:23:02,040
경우에 따라 L1 사이즈를 손해보지만, L2를 엄청나게 빠르게 해 줍니다

426
00:23:02,040 --> 00:23:03,440
저희에겐 굉장히 좋은거죠

427
00:23:03,560 --> 00:23:08,580
한대의 서버에는 수많은 RocksDB 인스턴스가 있습니다

428
00:23:08,600 --> 00:23:11,440
이를 통해 지연시간 역시 줄일 수 있습니다

429
00:23:12,440 --> 00:23:14,860
사전-컴퓨팅 사례에서는 대단히 좋습니다만

430
00:23:14,900 --> 00:23:22,280
현재 우린 더 많은 종류의 데이터 처리 문제를 해결하고 있습니다

431
00:23:23,040 --> 00:23:27,660
예를 들어 아주 자주 변경되는 데이터 뒤에 아주 천천히 변경되는 데이터가 FIFO 안에 있다면

432
00:23:27,740 --> 00:23:31,180
자주 변경되는 데이터는 디스크에 모두 저장되겠지만

433
00:23:31,180 --> 00:23:33,000
이 동작이 느리게 변경되는 큐 안의 데이터를 밀어내게 되어

434
00:23:33,000 --> 00:23:34,540
어떤 데이터는 잃어버릴 가능성도 있습니다

435
00:23:34,540 --> 00:23:35,760
좋지 않은 일이죠

436
00:23:35,840 --> 00:23:37,840
아직 이 문제는 해결하는 중입니다

437
00:23:39,520 --> 00:23:41,520
모든것을 함께 보자면

438
00:23:41,520 --> 00:23:43,260
조금 더 복잡해 집니다

439
00:23:43,360 --> 00:23:47,620
서로 다른 사용성을 위해 시스템 안에 여러개의 포트를 사용합니다

440
00:23:47,620 --> 00:23:48,720
서버 한대 안에서요

441
00:23:48,800 --> 00:23:54,040
그리고 이 서버들은 이미 개인화 데이터를 서비스 하고 있는 서버들입니다

442
00:23:54,180 --> 00:23:56,860
이전에 설명했던 개인화 파이프라인을 비롯해서

443
00:23:56,860 --> 00:24:00,500
EVCache 노드들이 이 서버에서 동작합니다

444
00:24:01,860 --> 00:24:03,860
Rend는 현재 두개의 포트로 운용중인데

445
00:24:04,000 --> 00:24:08,600
하나는 매우 동적인 접근이나 적극적으로 관리되는 데이터를 위한 것이고

446
00:24:08,740 --> 00:24:11,180
다른 하나는 비동기 배치를 위한 포트입니다

447
00:24:11,340 --> 00:24:15,020
음, 사실 배치를 위한 것이지 비동기라고 하긴 힘듭니다만

448
00:24:15,140 --> 00:24:19,920
덜 자주 접근되는 데이터 쓰기를 위한 포트입니다

449
00:24:20,420 --> 00:24:22,760
그리고 자주 사용되는 데이터를 계속 (핫하게) 유지해야 하는데

450
00:24:22,940 --> 00:24:27,780
개인화 작업 수행시 L1 캐쉬미스 대한 문제를 방지할 수 있기때문입니다

451
00:24:29,240 --> 00:24:30,820
좋습니다

452
00:24:30,820 --> 00:24:32,200
제가 기다렸던 슬라이드네요

453
00:24:34,460 --> 00:24:37,920
이건 넷플릭스에서 가장 격렬하게 사용되는

454
00:24:38,340 --> 00:24:41,400
모네타 캐싱 서비스에 대한 지표인데요

455
00:24:42,520 --> 00:24:46,280
이 데이터는 3시간의 피크동안 수집된 것이고

456
00:24:46,400 --> 00:24:48,400
가장 높은값과 가장 낮은 값을 모두 포함하고 있습니다

457
00:24:49,060 --> 00:24:51,960
서로 다른 많은 지연시간과 퍼센트들 같은 데이터 입니다

458
00:24:53,760 --> 00:24:56,160
보셨던 모든 복잡성을 생각해 보세요

459
00:24:56,280 --> 00:24:59,540
아마도 여러분은 이거 엄청 느릴수도 있겠다 라고 생각하시겠지만

460
00:24:59,760 --> 00:25:01,760
여기 GET에 대한 성능을 보면

461
00:25:01,880 --> 00:25:06,060
우리가 이룬 평균 지연시간은 서버측에서 230 마이크로초 정도 입니다

462
00:25:06,560 --> 00:25:08,560
그리고 퍼센트 정보도 보실수 있는데요

463
00:25:08,700 --> 00:25:12,380
가장 높은 부분은 Go가 가비지 컬렉팅 언어기 때문에 발생한거구요

464
00:25:12,420 --> 00:25:14,420
그렇지만 충분히 납득할 만한 수치입니다

465
00:25:14,500 --> 00:25:17,640
99번째는 정말 굉장한데요

466
00:25:17,640 --> 00:25:19,640
그 아래에서는 심지어 더 훨씬 빠르죠

467
00:25:20,140 --> 00:25:23,700
SET 역시 매우 만족할 만한 수치를 보여줍니다

468
00:25:23,700 --> 00:25:26,280
평균 367 마이크로 세컨드로 측정 되었고

469
00:25:26,640 --> 00:25:30,620
앞에서와 마찬가지로 퍼센트 지표를 보실 수 있습니다

470
00:25:30,620 --> 00:25:32,980
이 모든 지표는 우리에겐 대단한 것이었습니다

471
00:25:33,100 --> 00:25:37,980
실제 클라이언트는 250-500 마이크로세컨의 AWS 네트워크 환경의 영향을 받으므로

472
00:25:38,060 --> 00:25:41,940
대부분의 사람들은 이 새로운 캐시로 부터의 응답을 1밀리초 이내에 받습니다

473
00:25:41,940 --> 00:25:43,500
거의 '항상' 그렇습니다

474
00:25:46,380 --> 00:25:48,380
좋습니다

475
00:25:48,480 --> 00:25:50,980
아까 말씀드렸듯이, 모네타는 비용 절감 프로젝트로 시작했습니다

476
00:25:50,980 --> 00:25:52,560
추측해 보세요

477
00:25:52,680 --> 00:25:55,660
한대의 클러스터가 절감한 비용은 몇 퍼센트일까요?

478
00:25:58,620 --> 00:26:00,220
1000%

479
00:26:00,220 --> 00:26:01,720
어, 그건 불가능한 수치죠

480
00:26:01,720 --> 00:26:04,460
[청중 웃음]

481
00:26:04,720 --> 00:26:06,720
10%, 약간 낮은 수치네요

482
00:26:06,780 --> 00:26:09,060
네. 70% 의 비용을 절감했습니다

483
00:26:09,060 --> 00:26:10,740
단 하나의 클러스터 기준으로요

484
00:26:10,740 --> 00:26:15,380
설명했듯이, 항상 사용되는 데이터와 잘 사용되지 않는 데이터의 분리를 통해서요

485
00:26:16,100 --> 00:26:18,580
이 프로젝트는 넷플릭스로서는 정말 대단한 결과물입니다

486
00:26:18,860 --> 00:26:21,720
이 도구는 곧 프로덕션을 앞두고 있습니다

487
00:26:22,040 --> 00:26:26,080
네. 넷플릭스로는 큰 성공이 될겁니다

488
00:26:26,080 --> 00:26:28,040
오늘 보신 모든 것들은

489
00:26:28,040 --> 00:26:29,360
아 모든건 아니군요

490
00:26:29,480 --> 00:26:31,480
'니모닉'은 제외되죠

491
00:26:31,740 --> 00:26:34,780
EVCache, 자바 클라이언트 라이브러리

492
00:26:34,780 --> 00:26:36,260
오픈소스구요

493
00:26:36,380 --> 00:26:40,660
그리고 앞서 설명한 REST API 역시 오픈소스고, 동일한 레포에 있습니다

494
00:26:41,000 --> 00:26:46,080
Rend 역시 넷플릭스 깃헙에 오픈소스로 존재합니다

495
00:26:47,460 --> 00:26:49,460
가서 살펴보시면 좋을것 같아요

496
00:26:50,020 --> 00:26:52,020
여기까지가 제가 발표할 전부 입니다

497
00:26:53,260 --> 00:27:01,400
[청중 박수]

498
00:27:03,300 --> 00:27:06,840
질문 받을께요. 누구든... 네

499
00:27:07,060 --> 00:27:16,160
[뭐라고 뭐라고 질문]

500
00:27:16,900 --> 00:27:21,900
어 질문은 'RocksDB' 사용에 대한 결정 과정에 대한 것인데요

501
00:27:23,460 --> 00:27:27,720
음 사실 제가 바로 그 RocksDB 사용 결정을 내린 엔지니어 입니다

502
00:27:27,780 --> 00:27:30,720
음, 하지만

503
00:27:30,720 --> 00:27:33,840
우리의 사용 조건에 가장 빠른 성능을 보여줬구요

504
00:27:37,940 --> 00:27:40,560
음.. (대답하려면) 생각을 좀 더 해봐야 겠네요

505
00:27:40,580 --> 00:27:44,140
음, 세션후에 부스에 몇시간 있을거니까 찾아와 주세요

506
00:27:45,460 --> 00:27:46,640
네

507
00:27:51,740 --> 00:27:54,160
네, 질문은 CouchBase도 고려 했었는가 입니다

508
00:27:54,160 --> 00:27:56,060
대답은 '아니오'입니다

509
00:27:56,060 --> 00:27:57,980
[웃음]

510
00:27:58,380 --> 00:28:00,700
이야기 한 적은 있습니다만

511
00:28:00,800 --> 00:28:04,160
우리가 직접 만든 도구가 우리에게 적합하다고 생각합니다

512
00:28:04,280 --> 00:28:06,360
어떤 도구든 어쨌든 커스터마이즈 해야 하구요

513
00:28:06,400 --> 00:28:09,560
원하는 기능을 코드로 만들어서 JAR로 내고

514
00:28:09,560 --> 00:28:10,620
하루만에요

515
00:28:10,640 --> 00:28:13,940
이런 우리의 프로세스가 CouchBase에 적합하지 않다고 생각합니다

516
00:28:13,940 --> 00:28:15,300
그래서

517
00:28:16,520 --> 00:28:22,380
이 부분에서 깊게 이야기한적은 없지만, 지금 우리가 가진것에 만족합니다

518
00:28:22,560 --> 00:28:28,720
그리고 지금 보신 것들을 말씀하신 도구로 할 수 있다고 생각하지 않습니다

519
00:28:29,480 --> 00:28:31,480
네

520
00:28:32,220 --> 00:28:40,240
[뭐라고뭐라고 질문]

521
00:28:40,640 --> 00:28:42,640
어...

522
00:28:42,940 --> 00:28:44,940
[뭐라고 뭐라고 질문 계속]

523
00:28:45,000 --> 00:28:50,680
네. 질문은, memcached 가 제공하는 커맨드 세트가 부족하진 않았나요 입니다

524
00:28:52,220 --> 00:28:54,220
음, 부족하지는 않았습니다

525
00:28:54,260 --> 00:28:58,960
우리의 대부분의 데이터는 사용자 프로파일과 같은 것들이고

526
00:28:58,960 --> 00:29:02,340
대부분의 경우 사용자들은 전체 데이터세트를 원했습니다

527
00:29:02,380 --> 00:29:06,620
배열에서 단 하나의 엘레먼트만을 요구하는 사용 사례는 많지 않았구요

528
00:29:06,620 --> 00:29:08,260
음, 그래서

529
00:29:08,260 --> 00:29:10,160
더 많은 커맨드가 필요하진 않았습니다

530
00:29:13,380 --> 00:29:16,120
다른 질문 더 있나요? 네

531
00:29:16,560 --> 00:29:21,760
[뭐라고뭐라고 질문]

532
00:29:22,260 --> 00:29:25,700
질문은, 프로젝트 타임라인, 모네타 말씀하시는거죠?

533
00:29:26,280 --> 00:29:28,440
어, 타임 라인

534
00:29:28,760 --> 00:29:32,980
음, 저는 Go를 약 일년 전부터 사용하기 시작했는데요

535
00:29:33,740 --> 00:29:37,020
rent로 이것저것 하기 시작한게 아마

536
00:29:37,960 --> 00:29:39,960
11월쯤 되었을 겁니다

537
00:29:41,280 --> 00:29:48,500
그리고 나서 올 초에 저희 팀 동료를 지금 보신 방식으로 프로젝트를 진행하자고 설득했는데요

538
00:29:48,840 --> 00:29:55,340
음, 그리고 가장 큰 서비스에 적용을 시작한게 바로 요즘 입니다

539
00:29:55,360 --> 00:29:57,360
음 그래서 전체적으로 보면 1년 정도?

540
00:29:59,240 --> 00:30:01,240
네 거기

541
00:30:01,440 --> 00:30:20,360
[뭐라고뭐라고 질문]

542
00:30:20,620 --> 00:30:23,960
어, 제가 어떻게 Go를 쓰자고 팀 동료를 설득했냐구요

543
00:30:24,180 --> 00:30:27,680
음, 왜냐면 우린 더 이상 포장도로를 달리는게 아니거든요
(새로운 방법을 찾아 힘든길을 가야한다는 의미)

544
00:30:30,080 --> 00:30:34,680
사실 컨셉을 확인하고 나면 그다지 어려운일은 아닙니다

545
00:30:34,700 --> 00:30:38,380
말씀드렸다시피 매우 빠르구요. 컴파일 버전은 더 빠릅니다

546
00:30:38,500 --> 00:30:45,640
상대적으로 멍청한 텍스트 프로토콜 버전이 이미 수백만의 요청을 처리하고 있기도 했고

547
00:30:45,900 --> 00:30:47,900
그래서 엄

548
00:30:48,340 --> 00:30:50,960
당시에는 그정도로 빠를 필요가 없었고

549
00:30:51,100 --> 00:30:53,100
그저 한대의 서버가 필요했었구요

550
00:30:53,300 --> 00:30:55,740
음, 그래서 거기서 시작한거죠

551
00:30:55,740 --> 00:30:57,880
서버에서 어떻게 이걸 돌아가게 할 지

552
00:30:58,000 --> 00:31:02,200
사실 이미 memcached 라는 바이너리 blob가 있었고

553
00:31:02,320 --> 00:31:05,960
그리고 이미 비포장 도로를 달리고 있어서, 그다지 두렵진 않았습니다

554
00:31:07,020 --> 00:31:10,380
음 그리고 더하자면 우리 팀 동료들은

555
00:31:11,000 --> 00:31:13,700
지금은 종교를 Go 로 바꿨구요

556
00:31:13,780 --> 00:31:16,560
이 새로운 언어를 매우 좋아하는것 같아요

557
00:31:19,400 --> 00:31:21,960
다른 질문 더 있나요

558
00:31:21,960 --> 00:31:23,200
네

559
00:31:23,800 --> 00:31:30,960
[뭐라고뭐라고 질문]

560
00:31:31,160 --> 00:31:34,880
음, 질문은, 매니지먼트 팀이 비용을 절감하라는 지시를 한건가요

561
00:31:34,880 --> 00:31:36,860
직접적으로 요청했던것은 아닙니다

562
00:31:37,500 --> 00:31:41,300
넷플릭스의 엔지니어들은 우리의 사업이 어떤 방향으로 진행되고 있는지,

563
00:31:41,300 --> 00:31:44,700
필요한 것이 무엇인지 알고있습니다

564
00:31:44,740 --> 00:31:46,740
팀으로서

565
00:31:46,820 --> 00:31:52,020
우리는 어떤 부분을 최적화 해야 하는지 찾았던거구요

566
00:31:52,480 --> 00:31:54,480
그래서 찾았고, 진행하기로 했습니다

567
00:31:54,780 --> 00:31:58,230
그리고 우리 스스로 뿐만 아니라 데이터를 저장하고 있는

568
00:31:58,230 --> 00:32:02,140
다른 팀들에게도 이게 올바른 방법이라는 점을 설득해야 했습니다

569
00:32:02,360 --> 00:32:06,960
음 그 팀에 가서, "우리 데이터 저장 방법을 조금 바꿀건데 괜찮겠어?"

570
00:32:07,160 --> 00:32:09,600
음 그리고 대부분 동의한거죠

571
00:32:10,000 --> 00:32:15,020
음, 그래서 대부분은 엔지니어들에 의해 진행된 거지만

572
00:32:15,100 --> 00:32:18,040
이 아이디어를 제안하는 동안 백업도 있기는 했습니다

573
00:32:18,320 --> 00:32:23,260
[뭐라고뭐라고 질문]

574
00:32:23,500 --> 00:32:30,660
음, 사이드 프로젝트로 시작해서 메인 프로젝트가 된건데, 간단합니다. 이게 우선순위가 높았어요

575
00:32:32,760 --> 00:32:34,760
다른 질문 더 있나요?

576
00:32:34,760 --> 00:32:35,860
네

577
00:32:36,320 --> 00:32:43,420
[뭐라고뭐라고 질문]

578
00:32:44,240 --> 00:32:46,240
어 잘안들리는데 좀 더 크게 말씀해 주시겠어요?

579
00:32:46,940 --> 00:33:07,540
[뭐라고뭐라고 다시 질문]

580
00:33:08,020 --> 00:33:13,220
음 네. 질문하신 부분을 '잘' 설명 하지 못한것 같은데요

581
00:33:13,360 --> 00:33:15,880
공식 행사에서 발표를 하는것은 이번이 처음이라서요

582
00:33:16,060 --> 00:33:19,480
넷플릭스의 개인화 프로세스는

583
00:33:19,520 --> 00:33:24,060
ad-hoc 으로 의존성 있는 데이터를 결과물로 내 놓습니다

584
00:33:24,380 --> 00:33:27,320
음 하지만 이 데이터들은 어떤 정형화된

585
00:33:27,480 --> 00:33:30,540
어떤 약속된 형태인 것은 아니며

586
00:33:30,540 --> 00:33:33,780
어떤 엔지니어가 '이건 내 애플리케이션에 의존성이 있는 데이터야' 하지 않는 경우라면 말이죠

587
00:33:34,340 --> 00:33:37,640
그래서 애플리케이션들이 (개인화 결과) 데이터를 저장할때

588
00:33:37,640 --> 00:33:39,260
그들의 프로세스는

589
00:33:39,380 --> 00:33:42,460
EVCache 클러스터에 저장하는 것입니다

590
00:33:42,680 --> 00:33:48,420
그리고 이 EVCache 클러스터들은 '디스커버리 서비스 시스템'을 통해서

591
00:33:48,680 --> 00:33:52,680
아무때나 다른 서비스에 의해 접근할 수 있습니다

592
00:33:53,060 --> 00:33:55,420
이게 질문에 답이 될지는 모르겠습니다만

593
00:33:56,620 --> 00:34:02,440
네. EVCache가 유레카 시스템을 사용하기 때문에 누구든 데이터에 접근할 수 있는 것이죠

594
00:34:02,900 --> 00:34:04,900
질문에 답이 되었나요?

595
00:34:05,300 --> 00:34:14,520
[뭐라고뭐라고 또 질문]

596
00:34:14,740 --> 00:34:19,660
음 그 그림에서요, 녹색의 데이터베이스 같은 것도 있고

597
00:34:19,660 --> 00:34:23,160
그 그림이 전체 클러스터의 모습을 표현한겁니다

598
00:34:23,220 --> 00:34:26,340
글로벌로 복제되는 샤딩된 클러스터인거죠

599
00:34:27,360 --> 00:34:29,360
음 다른 질문자 어 네

600
00:34:31,880 --> 00:34:34,920
다른 클라우드 사업자로 확장할 계획은 없나요?

601
00:34:35,160 --> 00:34:37,160
음... 이건 오픈소스죠

602
00:34:37,220 --> 00:34:38,480
[청중 웃음]

603
00:34:38,480 --> 00:34:39,740
패치도 받습니다 [웃음]

604
00:34:39,740 --> 00:34:41,220
pull request 해 주세요

605
00:34:41,260 --> 00:34:43,540
음, 그럴 계획은 없습니다

606
00:34:43,560 --> 00:34:46,900
우리 팀은 넷플릭스의 문제를 해결하는데 집중하고 있습니다

607
00:34:47,260 --> 00:34:51,680
우리의 오픈소스가 다른 분들의 문제도 해결하면 좋겠지만

608
00:34:52,060 --> 00:34:57,240
오픈 소스 자체를 지원하는 것이 우리 팀의 목표는 아닙니다

609
00:34:57,400 --> 00:35:00,620
음, 하지만 누군가에는 유용할겁니다

610
00:35:01,940 --> 00:35:03,940
더 없나요?

611
00:35:04,740 --> 00:35:05,840
탑탑

612
00:35:14,260 --> 00:35:17,140
끝인것 같네요

